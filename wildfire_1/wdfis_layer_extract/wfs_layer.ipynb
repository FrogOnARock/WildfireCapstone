{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*****\n",
    "## WFS Layer Extract\n",
    "*****\n",
    "Author: Mackenzie Rock\n",
    "\n",
    "Date: June 3, 2025\n",
    "\n",
    "Goal: The goal of this Jupyter notebook is to determine a foundation for extracting the relevant WFS layers identified and storing them in a suitable format for uploading to the postgreSQL database. In this section I will test:\n",
    "- The code to extract\n",
    "- I will visualize the data to ensure I have capture it correctly\n",
    "- I will test several samples overtime\n",
    "- Transformation into suitable format for load\n",
    "- Examination of some of the datasets\n",
    "- Framework for inserting into Google Cloud PostgreSQL database\n",
    "\n",
    "### 1.0 Extraction Code\n",
    "\n",
    "To be extracted from WFS:\n",
    "- Fire Danger (PUBLIC:FDR_CURRENT_SHP)\n",
    "- Fire Perimeter Estimate (PUBLIC:M3_POLYGONS_CURRENT)\n",
    "- Fire M3 Hotspots (PUBLIC:HOTSPOTS_LAST24HRS)\n",
    "- Season-to-date Hotspots (TBD)\n",
    "- Active Fires (PUBLIC:ACTIVEFIRES_CURRENT)\n",
    "- Forecast Weather Stations (PUBLIC:FIREWX_STNS & PUBLIC:FIREWX_STNS_CURRENT)\n",
    "- Reporting Weather Stations (PUBLIC:FIREWX_SCRIBE & PUBLIC:FIREWX_SCRIBE_FCST)\n",
    "- Fire History (PUBLIC:REPORTEDFIRES_2024 & PUBLIC:REPORTEDFIRES_YTD)\n",
    "- Check on what PUBLIC:BASEMAP_INSIDE_BNDRY & PUBLIC:BASEMAP_LAND are"
   ],
   "id": "f0bcfa639c6fa3f3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T23:41:13.752596Z",
     "start_time": "2025-06-06T23:41:13.745073Z"
    }
   },
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import mapping\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# WFS endpoint\n",
    "WFS_URL = \"https://cwfis.cfs.nrcan.gc.ca/geoserver/public/ows\"\n",
    "OUTPUT_DIR = \"./wfs_layers\"\n",
    "MAP_OUTPUT_DIR = \"wfs_maps\"\n",
    "\n",
    "# Ensure output dirs exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MAP_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_visualize_wfs_layer(layer_name: str, label: str = None, date: str = None, output_format: str = \"GeoJSON\"):\n",
    "    \"\"\"\n",
    "    Fetches a layer from the CWFIS WFS endpoint, reprojects to EPSG:4326,\n",
    "    saves to file, and visualizes using Folium.\n",
    "    \"\"\"\n",
    "    # Build WFS URL\n",
    "    params = {\n",
    "        \"services\": \"WFS\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"request\": \"GetFeature\",\n",
    "        \"typename\": layer_name,\n",
    "        \"outputFormat\": \"application/json\"\n",
    "    }\n",
    "    query_url = f\"{WFS_URL}?{'&'.join([f'{k}={v}' for k, v in params.items()])}\"\n",
    "\n",
    "    print(f\"Fetching {layer_name} from WFS...\")\n",
    "\n",
    "\n",
    "    # Read GeoDataFrame\n",
    "    response = requests.get(query_url)\n",
    "    response.raise_for_status()  # Fail early if bad response\n",
    "    gdf = gpd.read_file(BytesIO(response.content))\n",
    "\n",
    "    # Reproject if needed\n",
    "    if gdf.crs and gdf.crs.to_string() != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "\n",
    "    json_safe_gdf = gdf.copy()\n",
    "    for col in json_safe_gdf.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(json_safe_gdf[col]) or isinstance(json_safe_gdf[col].iloc[0], pd.Timestamp):\n",
    "            print(f\"Converting datetime column '{col}' to string\")\n",
    "            json_safe_gdf[col] = json_safe_gdf[col].astype(str)\n",
    "        elif isinstance(json_safe_gdf[col].iloc[0], (list, dict)):\n",
    "            json_safe_gdf[col] = json_safe_gdf[col].astype(str)\n",
    "\n",
    "\n",
    "    # Save GeoJSON\n",
    "    label_safe = label.replace(\" \", \"_\").lower() if label else layer_name.replace(\":\", \"_\").lower()\n",
    "    filename = f\"{label_safe}_{date or 'latest'}.{output_format.lower()}\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    json_safe_gdf.to_file(filepath, driver=\"GeoJSON\")\n",
    "\n",
    "    # Create map centered on centroid of geometry\n",
    "    centroid = json_safe_gdf.geometry.unary_union.centroid\n",
    "    fmap = folium.Map(location=[centroid.y, centroid.x], zoom_start=5)\n",
    "\n",
    "    # Add layer as external file read\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        folium.GeoJson(data=f.read(), name=label or layer_name).add_to(fmap)\n",
    "\n",
    "    # Save map\n",
    "    html_map_path = os.path.join(MAP_OUTPUT_DIR, f\"{label_safe}_{date or 'latest'}.html\")\n",
    "    fmap.save(html_map_path)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T23:41:42.891893Z",
     "start_time": "2025-06-06T23:41:15.067080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from wfs_utils import fetch_and_visualize_wfs_layer\n",
    "\n",
    "map_output_dir = \"wfs_maps\"\n",
    "layer_names = {\n",
    "    \"Fire_Danger\": \"public:fdr_current_shp\",\n",
    "    \"Fire_Perimeter_Estimate\": \"public:m3_polygons_current\",\n",
    "    \"M3_Hotspots\": \"public:hotspots_last24hrs\",\n",
    "    \"Active_Fires\": \"public:activefires_current\",\n",
    "    \"Forecast_Weather_Stations\": \"public:firewx_stns\",\n",
    "    \"Forecast_Weather_Stations_Current\": \"public:firewx_stns_current\",\n",
    "    \"Reporting_Weather_Stations\": \"public:firewx_scribe\",\n",
    "    \"Reporting_Weather_Stations_Forecast\": \"public:firewx_scribe_fcst\",\n",
    "    \"Fire_History_YTD\": \"public:reportedfires_ytd\",\n",
    "    \"Fire_History_2024\": \"public:reportedfires_2024\"\n",
    "}\n",
    "dates = [\"2024-01-01\"]\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    for label, type in layer_names.items():\n",
    "\n",
    "        print(f\"Attempting to process layer: {label}\")\n",
    "        fetch_and_visualize_wfs_layer(type, label, date)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b7ddeda122d3b69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: Fire_Danger\n",
      "Fetching public:fdr_current_shp from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: Fire_Perimeter_Estimate\n",
      "Fetching public:m3_polygons_current from WFS...\n",
      "Converting datetime column 'firstdate' to string\n",
      "Converting datetime column 'lastdate' to string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: M3_Hotspots\n",
      "Fetching public:hotspots_last24hrs from WFS...\n",
      "Converting datetime column 'rep_date' to string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: Active_Fires\n",
      "Fetching public:activefires_current from WFS...\n",
      "Converting datetime column 'startdate' to string\n",
      "Attempting to process layer: Forecast_Weather_Stations\n",
      "Fetching public:firewx_stns from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime column 'rep_date' to string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: Forecast_Weather_Stations_Current\n",
      "Fetching public:firewx_stns_current from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Reporting_Weather_Stations\n",
      "Fetching public:firewx_scribe from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Reporting_Weather_Stations_Forecast\n",
      "Fetching public:firewx_scribe_fcst from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Fire_History_YTD\n",
      "Fetching public:reportedfires_ytd from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime column 'startdate' to string\n",
      "Attempting to process layer: Fire_History_2024\n",
      "Fetching public:reportedfires_2024 from WFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime column 'startdate' to string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36544/2130008081.py:64: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = json_safe_gdf.geometry.unary_union.centroid\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Format for Stoage\n",
    "- I will store the data in GPKG for storage reasons. This data will be extracted and transformed to GeoJSON for visualization"
   ],
   "id": "bb296c5a70813f02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.0 Assessing the reporting date within forecast weather stations\n",
    "\n",
    "I'm intentionally assessing the reporting data as I want to understand whether this is fully historical or snapshot data like some of the other WFS layers that I am pulling. It shows that it is historical data and the current is snapshot.\n",
    "\n",
    "Thus I will be inserting the historical and using current (the snapshot) to provide updates to my database daily."
   ],
   "id": "a51c5a273ff5663a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T23:05:11.663082Z",
     "start_time": "2025-06-06T23:04:54.744971Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process layer: Fire_Danger\n",
      "Fetching public:fdr_current_shp from WFS...\n",
      "Attempting to process layer: Fire_Perimeter_Estimate\n",
      "Fetching public:m3_polygons_current from WFS...\n",
      "Converting datetime column 'firstdate' to string\n",
      "Converting datetime column 'lastdate' to string\n",
      "Attempting to process layer: M3_Hotspots\n",
      "Fetching public:hotspots_last24hrs from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Active_Fires\n",
      "Fetching public:activefires_current from WFS...\n",
      "Converting datetime column 'startdate' to string\n",
      "Attempting to process layer: Forecast_Weather_Stations\n",
      "Fetching public:firewx_stns from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Forecast_Weather_Stations_Current\n",
      "Fetching public:firewx_stns_current from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Reporting_Weather_Stations\n",
      "Fetching public:firewx_scribe from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Reporting_Weather_Stations_Forecast\n",
      "Fetching public:firewx_scribe_fcst from WFS...\n",
      "Converting datetime column 'rep_date' to string\n",
      "Attempting to process layer: Fire_History_YTD\n",
      "Fetching public:reportedfires_ytd from WFS...\n",
      "Converting datetime column 'startdate' to string\n",
      "Attempting to process layer: Fire_History_2024\n",
      "Fetching public:reportedfires_2024 from WFS...\n",
      "Converting datetime column 'startdate' to string\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.geometry import mapping\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# WFS endpoint\n",
    "WFS_URL = \"https://cwfis.cfs.nrcan.gc.ca/geoserver/public/ows\"\n",
    "OUTPUT_DIR = \"./wfs_layers\"\n",
    "MAP_OUTPUT_DIR = \"wfs_maps\"\n",
    "\n",
    "# Ensure output dirs exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MAP_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch_and_visualize_wfs_layer(layer_name: str, label: str = None, date: str = None, output_format: str = \"\"):\n",
    "    \"\"\"\n",
    "    Fetches a layer from the CWFIS WFS endpoint, reprojects to EPSG:4326,\n",
    "    saves to file, and visualizes using Folium.\n",
    "    \"\"\"\n",
    "    # Build WFS URL\n",
    "    params = {\n",
    "        \"services\": \"WFS\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"request\": \"GetFeature\",\n",
    "        \"typename\": layer_name,\n",
    "        \"outputFormat\": \"application/json\"\n",
    "    }\n",
    "    query_url = f\"{WFS_URL}?{'&'.join([f'{k}={v}' for k, v in params.items()])}\"\n",
    "\n",
    "    print(f\"Fetching {layer_name} from WFS...\")\n",
    "\n",
    "    # Read GeoDataFrame\n",
    "    response = requests.get(query_url)\n",
    "    response.raise_for_status()  # Fail early if bad response\n",
    "    gdf = gpd.read_file(BytesIO(response.content))\n",
    "\n",
    "    # Reproject if needed\n",
    "    if gdf.crs and gdf.crs.to_string() != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    json_safe_gdf = gdf.copy()\n",
    "    for col in json_safe_gdf.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(json_safe_gdf[col]) or isinstance(json_safe_gdf[col].iloc[0],\n",
    "                                                                                  pd.Timestamp):\n",
    "            print(f\"Converting datetime column '{col}' to string\")\n",
    "            json_safe_gdf[col] = json_safe_gdf[col].astype(str)\n",
    "        elif isinstance(json_safe_gdf[col].iloc[0], (list, dict)):\n",
    "            json_safe_gdf[col] = json_safe_gdf[col].astype(str)\n",
    "\n",
    "    # Save gpkg\n",
    "    label_safe = label.replace(\" \", \"_\").lower() if label else layer_name.replace(\":\", \"_\").lower()\n",
    "    filename = f\"{label_safe}_{date or 'latest'}.{output_format.lower()}\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    gdf.to_file(f\"{filepath}gpkg\", layer=\"fire_perimeter\", driver=\"GPKG\")\n",
    "\n",
    "\n",
    "#from wfs_utils import fetch_and_visualize_wfs_layer\n",
    "\n",
    "map_output_dir = \"wfs_maps\"\n",
    "layer_names = {\n",
    "    \"Fire_Danger\": \"public:fdr_current_shp\",\n",
    "    \"Fire_Perimeter_Estimate\": \"public:m3_polygons_current\",\n",
    "    \"M3_Hotspots\": \"public:hotspots_last24hrs\",\n",
    "    \"Active_Fires\": \"public:activefires_current\",\n",
    "    \"Forecast_Weather_Stations\": \"public:firewx_stns\",\n",
    "    \"Forecast_Weather_Stations_Current\": \"public:firewx_stns_current\",\n",
    "    \"Reporting_Weather_Stations\": \"public:firewx_scribe\",\n",
    "    \"Reporting_Weather_Stations_Forecast\": \"public:firewx_scribe_fcst\",\n",
    "    \"Fire_History_YTD\": \"public:reportedfires_ytd\",\n",
    "    \"Fire_History_2024\": \"public:reportedfires_2024\"\n",
    "}\n",
    "dates = [\"2024-01-01\"]\n",
    "\n",
    "for date in dates:\n",
    "    for label, type in layer_names.items():\n",
    "        print(f\"Attempting to process layer: {label}\")\n",
    "        fetch_and_visualize_wfs_layer(type, label, date)\n",
    "\n",
    "\n"
   ],
   "id": "45c56a669458af41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quick check on the date ranges in forecast weather stations to see the data structure.",
   "id": "675dde4c48294e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Load your GeoJSON data\n",
    "with open(\"wfs_layers/forecast_weather_stations_2025-06-01.geojson\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Store min/max rep_date per station id\n",
    "station_dates = defaultdict(lambda: {\"min\": None, \"max\": None})\n",
    "\n",
    "for feature in data[\"features\"]:\n",
    "    station_id = feature[\"properties\"][\"id\"]\n",
    "    rep_date_str = feature[\"properties\"][\"rep_date\"]\n",
    "    rep_date = datetime.fromisoformat(rep_date_str.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "    if (station_dates[station_id][\"min\"] is None) or (rep_date < station_dates[station_id][\"min\"]):\n",
    "        station_dates[station_id][\"min\"] = rep_date\n",
    "    if (station_dates[station_id][\"max\"] is None) or (rep_date > station_dates[station_id][\"max\"]):\n",
    "        station_dates[station_id][\"max\"] = rep_date\n",
    "\n",
    "# Print summary\n",
    "for station_id, dates in station_dates.items():\n",
    "    print(f\"{station_id}: from {dates['min']} to {dates['max']}\")"
   ],
   "id": "ffb427804969b1ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T19:16:30.668032Z",
     "start_time": "2025-06-07T19:16:30.656344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Load your GeoJSON data\n",
    "with open(\"wfs_layers/forecast_weather_stations_current_2025-06-01.geojson\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Store min/max rep_date per station id\n",
    "station_dates = defaultdict(lambda: {\"min\": None, \"max\": None})\n",
    "\n",
    "for feature in data[\"features\"]:\n",
    "    station_id = feature[\"properties\"][\"id\"]\n",
    "    rep_date_str = feature[\"properties\"][\"rep_date\"]\n",
    "    rep_date = datetime.fromisoformat(rep_date_str.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "    if (station_dates[station_id][\"min\"] is None) or (rep_date < station_dates[station_id][\"min\"]):\n",
    "        station_dates[station_id][\"min\"] = rep_date\n",
    "    if (station_dates[station_id][\"max\"] is None) or (rep_date > station_dates[station_id][\"max\"]):\n",
    "        station_dates[station_id][\"max\"] = rep_date\n",
    "\n",
    "# Print summary\n",
    "current_count = 0\n",
    "\n",
    "for station_id, dates in station_dates.items():\n",
    "    if current_count > 5:\n",
    "        break\n",
    "    print(f\"{station_id}: from {dates['min']} to {dates['max']}\")\n",
    "    current_count += 1"
   ],
   "id": "ae68a4af93078df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firewx_stns_current.fid--4699bdee_1973debb200_-48ae: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n",
      "firewx_stns_current.fid--4699bdee_1973debb200_-48ad: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n",
      "firewx_stns_current.fid--4699bdee_1973debb200_-48ac: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n",
      "firewx_stns_current.fid--4699bdee_1973debb200_-48ab: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n",
      "firewx_stns_current.fid--4699bdee_1973debb200_-48aa: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n",
      "firewx_stns_current.fid--4699bdee_1973debb200_-48a9: from 2025-06-04 12:00:00+00:00 to 2025-06-04 12:00:00+00:00\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.0 Framework for Inserting into postgreSQL",
   "id": "d974e2ebc4e6b214"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This module handles the ingestion of wildfire-related geospatial datasets by loading .gpkg files, applying dataset-specific transformations, validating the data against strict schemas, and uploading the results to a PostGIS database. Each dataset type (e.g., active fires, fire danger, weather stations) has its own transformation function to standardize formats, cast numeric types, and handle missing values. Validation is performed using pandera with typed schemas that ensure consistency and enforce that each input is a valid GeoDataFrame. A central registry maps table names to their corresponding validators, allowing the ETL process to remain generic and extensible. This structure keeps transformation, validation, and persistence concerns separate, and allows the pipeline to be easily extended with new datasets by adding entries to the transformation and validator mappings.",
   "id": "c9dfa03004496c1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T00:38:15.563528Z",
     "start_time": "2025-06-08T00:38:15.550306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pandera import DataFrameModel, Field, check_types\n",
    "from pandera.typing import DataFrame, Series\n",
    "from shapely.geometry import base\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='pandera')\n",
    "\n",
    "### Fire Danger Schema and Validator ###\n",
    "class FireDangerSchema(DataFrameModel):  # ✅ correct\n",
    "    id: Series[str]\n",
    "    GRIDCODE: Series[int] = Field(nullable=True)\n",
    "    acquisition_date: Series[datetime]\n",
    "\n",
    "class FireDangerValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate (df: DataFrame[FireDangerSchema]) -> DataFrame[FireDangerSchema]:\n",
    "        #geometry checks\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "### Active Fires Schema and Validator ###\n",
    "class ActiveFiresSchema(DataFrameModel):  # ✅ correct\n",
    "    id: Series[str]\n",
    "    firename: Series[str]\n",
    "    acquisition_date: Series[datetime]\n",
    "    startdate: Series[datetime]\n",
    "    hectares: Series[float]\n",
    "    lat: Series[float]\n",
    "    lon: Series[float]\n",
    "    agency: Series[str]\n",
    "    stage_of_control: Series[str]\n",
    "    response_type: Series[str]\n",
    "\n",
    "\n",
    "class ActiveFiresValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate (df: DataFrame[ActiveFiresSchema]) -> DataFrame[ActiveFiresSchema]:\n",
    "        #geometry checks\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "### Fire History Schema and Validator ###\n",
    "class FireHistorySchema(DataFrameModel):  # ✅ correct\n",
    "    id: Series[str]\n",
    "    firename: Series[str]\n",
    "    startdate: Series[datetime]\n",
    "    hectares: Series[float]\n",
    "    lat: Series[float]\n",
    "    lon: Series[float]\n",
    "    agency: Series[str]\n",
    "    stage_of_control: Series[str]\n",
    "    response_type: Series[str]\n",
    "    cause: Series[str]\n",
    "\n",
    "class FireHistoryValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate (df: DataFrame[FireHistorySchema]) -> DataFrame[FireHistorySchema]:\n",
    "        #geometry checks\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "### Fire Perimeter Schema and Validator ###\n",
    "class FirePerimeterSchema(DataFrameModel):  # ✅ correct\n",
    "    id: Series[str]\n",
    "    acquisition_date: Series[datetime]\n",
    "    firstdate: Series[datetime]\n",
    "    lastdate: Series[datetime]\n",
    "    hcount: Series[int]\n",
    "    area: Series[float] = Field(nullable=True)\n",
    "\n",
    "class FirePerimeterValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate (df: DataFrame[FirePerimeterSchema]) -> DataFrame[FirePerimeterSchema]:\n",
    "        #geometry checks\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "### ForecastWeatherStations ###\n",
    "class ForecastWeatherStationsSchema(DataFrameModel):\n",
    "    id: Series[str]\n",
    "    rep_date: Series[datetime]\n",
    "    wmo: Series[int]\n",
    "    name: Series[str]\n",
    "    agency: Series[str]\n",
    "    ua: Series[str]\n",
    "    instr: Series[str]\n",
    "    prov: Series[str]\n",
    "    lat: Series[float]\n",
    "    lon: Series[float]\n",
    "    elev: Series[float]\n",
    "    temp: Series[float]\n",
    "    td: Series[float]\n",
    "    rh: Series[float]\n",
    "    ws: Series[float]\n",
    "    wg: Series[float] = Field(nullable=True)\n",
    "    wdir: Series[int]\n",
    "    pres: Series[float]\n",
    "    vis: Series[float]\n",
    "    rndays: Series[int]\n",
    "    precip: Series[float]\n",
    "    sog: Series[float]\n",
    "    ffmc: Series[float] = Field(nullable=True)\n",
    "    dmc: Series[float] = Field(nullable=True)\n",
    "    dc: Series[float] = Field(nullable=True)\n",
    "    bui: Series[float] = Field(nullable=True)\n",
    "    isi: Series[float] = Field(nullable=True)\n",
    "    fwi: Series[float] = Field(nullable=True)\n",
    "    dsr: Series[float] = Field(nullable=True)\n",
    "\n",
    "class ForecastWeatherStationsValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate(df: DataFrame[ForecastWeatherStationsSchema]) -> DataFrame[ForecastWeatherStationsSchema]:\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "\n",
    "### ReportingWeatherStations ###\n",
    "class ReportingWeatherStationsSchema(DataFrameModel):\n",
    "    id: Series[str]\n",
    "    rep_date: Series[datetime]\n",
    "    wmo: Series[int]\n",
    "    name: Series[str]\n",
    "    latitude: Series[float]\n",
    "    longitude: Series[float]\n",
    "    elevation: Series[float]\n",
    "    temp: Series[float]\n",
    "    rh: Series[float]\n",
    "    ws: Series[float]\n",
    "    wdir: Series[float]\n",
    "    precip: Series[float]\n",
    "    sog: Series[float]\n",
    "    ffmc: Series[float]\n",
    "    dmc: Series[float]\n",
    "    dc: Series[float]\n",
    "    isi: Series[float]\n",
    "    bui: Series[float]\n",
    "    fwi: Series[float]\n",
    "    dsr: Series[float]\n",
    "    wx: Series[float]\n",
    "    wy: Series[float]\n",
    "    timezone: Series[int]\n",
    "    x: Series[float]\n",
    "    y: Series[float]\n",
    "\n",
    "class ReportingWeatherStationsValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate(df: DataFrame[ReportingWeatherStationsSchema]) -> DataFrame[ReportingWeatherStationsSchema]:\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "\n",
    "### ForecastWeatherStationsForecast ###\n",
    "class WeatherStationsForecastSchema(DataFrameModel):\n",
    "    id: Series[str]\n",
    "    rep_date: Series[datetime]\n",
    "    wmo: Series[int]\n",
    "    name: Series[str]\n",
    "    latitude: Series[float]\n",
    "    longitude: Series[float]\n",
    "    elevation: Series[float]\n",
    "    temp: Series[float]\n",
    "    rh: Series[float]\n",
    "    ws: Series[float]\n",
    "    wdir: Series[float]\n",
    "    precip: Series[float]\n",
    "    sog: Series[float]\n",
    "    ffmc: Series[float]\n",
    "    dmc: Series[float]\n",
    "    dc: Series[float]\n",
    "    isi: Series[float]\n",
    "    bui: Series[float]\n",
    "    fwi: Series[float]\n",
    "    dsr: Series[float]\n",
    "    wx: Series[float]\n",
    "    wy: Series[float]\n",
    "    timezone: Series[int]\n",
    "    x: Series[float]\n",
    "    y: Series[float]\n",
    "\n",
    "class WeatherStationsForecastValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate(df: DataFrame[WeatherStationsForecastSchema]) -> DataFrame[WeatherStationsForecastSchema]:\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "\n",
    "### M3Hotspots ###\n",
    "class M3HotspotsSchema(DataFrameModel):\n",
    "    id: Series[str]\n",
    "    rep_date: Series[datetime]\n",
    "    lat: Series[float]\n",
    "    lon: Series[float]\n",
    "    source: Series[str]\n",
    "    sensor: Series[str]\n",
    "    satellite: Series[str] = Field(nullable=True)\n",
    "    agency: Series[str]\n",
    "    temp: Series[float]\n",
    "    rh: Series[float]\n",
    "    ws: Series[float]\n",
    "    wd: Series[int]\n",
    "    pcp: Series[float]\n",
    "    elev: Series[float] = Field(nullable=True)\n",
    "    ffmc: Series[float]\n",
    "    dmc: Series[float]\n",
    "    dc: Series[float]\n",
    "    isi: Series[float]\n",
    "    bui: Series[float]\n",
    "    fwi: Series[float]\n",
    "    fuel: Series[str] = Field(nullable=True)\n",
    "    ros: Series[float]\n",
    "    sfc: Series[float]\n",
    "    tfc: Series[float]\n",
    "    tfc0: Series[float]\n",
    "    sfc0: Series[float]\n",
    "    bfc: Series[float]\n",
    "    hfi: Series[float]\n",
    "    cfb: Series[float]\n",
    "    cbh: Series[float] = Field(nullable=True)\n",
    "    cfl: Series[float] = Field(nullable=True)\n",
    "    pcuring: Series[float]\n",
    "    pconif: Series[float]\n",
    "    cfactor: Series[float] = Field(nullable=True)\n",
    "    greenup: Series[int]\n",
    "    ecozone: Series[str] = Field(nullable=True)\n",
    "    ecozona2: Series[str] = Field(nullable=True)\n",
    "    estarea: Series[float] = Field(nullable=True)\n",
    "    estarea2: Series[float] = Field(nullable=True)\n",
    "    estarea3: Series[float]\n",
    "    polyid: Series[str] = Field(nullable=True)\n",
    "    age: Series[int]\n",
    "    sfl: Series[float] = Field(nullable=True)\n",
    "    frp: Series[float]\n",
    "    times_burned: Series[int] = Field(nullable=True)\n",
    "\n",
    "class M3HotspotsValidator:\n",
    "    @staticmethod\n",
    "    @check_types\n",
    "    def validate(df: DataFrame[M3HotspotsSchema]) -> DataFrame[M3HotspotsSchema]:\n",
    "        assert isinstance(df, gpd.GeoDataFrame), \"Not a GeoDataFrame\"\n",
    "        return df\n",
    "\n",
    "\n",
    "# Schema registry\n",
    "VALIDATOR_REGISTRY = {\n",
    "    \"fire_danger\": FireDangerValidator,\n",
    "    'active_fires': ActiveFiresValidator,\n",
    "    'fire_history': FireHistoryValidator,\n",
    "    'fire_perimeter_estimates': FirePerimeterValidator,\n",
    "    'forecast_weather_stations': ForecastWeatherStationsValidator,\n",
    "    'm3_hotspots': M3HotspotsValidator,\n",
    "    'reporting_weather_stations': ReportingWeatherStationsValidator,\n",
    "    'reporting_weather_stations_forecast': WeatherStationsForecastValidator\n",
    "}\n",
    "\n"
   ],
   "id": "e1d5cdba6330665a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T00:49:31.295232Z",
     "start_time": "2025-06-08T00:49:03.348279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine\n",
    "from geoalchemy2 import Geometry\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#validate table using registry\n",
    "def validate_table(df, table_name):\n",
    "    validator = VALIDATOR_REGISTRY.get(table_name)\n",
    "    if validator is None:\n",
    "        raise ValueError(f\"No validator found for table {table_name}\")\n",
    "    return validator.validate(df)\n",
    "\n",
    "#transform fire danger layer\n",
    "def fire_danger_transform(df):\n",
    "    df['GRIDCODE'] = df['GRIDCODE'].astype('int64')\n",
    "    return df\n",
    "\n",
    "#transform active fire layer\n",
    "def active_fire_transform(df):\n",
    "    df['startdate'] = pd.to_datetime(df['startdate']).dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "#transform fire history layer\n",
    "def fire_history_transform(df):\n",
    "    df['startdate'] = pd.to_datetime(df['startdate']).dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "#transform fire perimeter layer\n",
    "def fire_perimeter_transform(df):\n",
    "    df['firstdate'] = pd.to_datetime(df['firstdate']).dt.tz_localize(None)\n",
    "    df['lastdate'] = pd.to_datetime(df['lastdate']).dt.tz_localize(None)\n",
    "    df['hcount'] = df['hcount'].astype('int64')\n",
    "    return df\n",
    "\n",
    "#transform forecast weather layer\n",
    "def forecast_weather_transform(df):\n",
    "    df['rep_date'] = pd.to_datetime(df['rep_date']).dt.tz_localize(None)\n",
    "    df['wmo'] = df['wmo'].astype('int64')\n",
    "    df['wdir'] = df['wdir'].astype('int64')\n",
    "    df['rndays'] = df['rndays'].astype('int64')\n",
    "    df['elev'] = df['elev'].astype(float)\n",
    "    return df\n",
    "\n",
    "#transform M3 hotspot layer\n",
    "def M3_transform(df):\n",
    "    df['rep_date'] = pd.to_datetime(df['rep_date']).dt.tz_localize(None)\n",
    "    df['wd'] = df['wd'].astype('int64')\n",
    "    df['greenup'] = df['greenup'].astype('int64')\n",
    "    df['times_burned'] = df['times_burned'].fillna(0).astype('int64')\n",
    "    df['age'] = df['age'].astype('int64')\n",
    "    float_columns = [\n",
    "        \"lat\", \"lon\", \"temp\", \"rh\", \"ws\", \"pcp\", \"elev\", \"ffmc\", \"dmc\", \"dc\", \"isi\", \"bui\", \"fwi\",\n",
    "        \"ros\", \"sfc\", \"tfc\", \"tfc0\", \"sfc0\", \"bfc\", \"hfi\", \"cfb\", \"cbh\", \"cfl\", \"pcuring\", \"pconif\",\n",
    "        \"cfactor\", \"estarea\", \"estarea2\", \"estarea3\", \"sfl\", \"frp\"\n",
    "    ]\n",
    "    for col in float_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "#transform reporting weather layer\n",
    "def reporting_weather_transform(df):\n",
    "    df['rep_date'] = pd.to_datetime(df['rep_date']).dt.tz_localize(None)\n",
    "    df['wmo'] = df['wmo'].astype('int64')\n",
    "    df['timezone'] = df['timezone'].astype('int64')\n",
    "    float_columns = [\n",
    "    \"latitude\", \"longitude\", \"elevation\", \"temp\", \"rh\", \"ws\", \"wdir\", \"precip\",\n",
    "    \"sog\", \"ffmc\", \"dmc\", \"dc\", \"isi\", \"bui\", \"fwi\", \"dsr\", \"wx\", \"wy\", \"x\", \"y\"\n",
    "    ]\n",
    "    for col in float_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "#transform reporting weather forecast layer\n",
    "def reporting_weather_forecast_transform(df):\n",
    "    df['rep_date'] = pd.to_datetime(df['rep_date']).dt.tz_localize(None)\n",
    "    df['wmo'] = df['wmo'].astype('int64')\n",
    "    df['timezone'] = df['timezone'].astype('int64')\n",
    "    float_columns = [\n",
    "    \"latitude\", \"longitude\", \"elevation\", \"temp\", \"rh\", \"ws\", \"wdir\", \"precip\",\n",
    "    \"sog\", \"ffmc\", \"dmc\", \"dc\", \"isi\", \"bui\", \"fwi\", \"dsr\", \"wx\", \"wy\", \"x\", \"y\"\n",
    "    ]\n",
    "    for col in float_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "#load gpkg, apply transform, validate and upload to postgis\n",
    "def load_gpkg_to_postgis(gpkg_path: str, table_name: str, db_url='postgresql://postgres:K2><X*T$Jad#gQg2@34.23.205.32:5432/postgres'):\n",
    "\n",
    "    acquisition_date = None\n",
    "    if table_name in ['active_fires', 'fire_danger', 'fire_perimeter_estimates']:\n",
    "        acquisition_date = pd.Timestamp(datetime.today().date())\n",
    "\n",
    "\n",
    "    # Load the GeoPackage\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "\n",
    "    #define the transformation dictionary\n",
    "    transform_dict = {\n",
    "        'fire_danger': fire_danger_transform,\n",
    "        'active_fires': active_fire_transform,\n",
    "        'fire_history': fire_history_transform,\n",
    "        'fire_perimeter_estimates': fire_perimeter_transform,\n",
    "        'forecast_weather_stations': forecast_weather_transform,\n",
    "        'm3_hotspots': M3_transform,\n",
    "        'reporting_weather_stations': reporting_weather_transform,\n",
    "        'reporting_weather_stations_forecast': reporting_weather_forecast_transform\n",
    "    }\n",
    "\n",
    "    #attempt transformation otherwise continue\n",
    "    try:\n",
    "        gdf = transform_dict[table_name](gdf)\n",
    "    except:\n",
    "        print(f'No transformations for {table_name}')\n",
    "        pass\n",
    "\n",
    "    # Add acquisition_date if applicable\n",
    "    if acquisition_date:\n",
    "        print(acquisition_date)\n",
    "        gdf[\"acquisition_date\"] = acquisition_date\n",
    "\n",
    "    # Ensure CRS is EPSG:4326\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    validated_gdf =  validate_table(gdf, table_name)\n",
    "\n",
    "    if validated_gdf.empty:\n",
    "        raise ValueError(\"GeoDataFrame is empty after validation.\")\n",
    "\n",
    "\n",
    "    # Connect to DB\n",
    "    engine = create_engine(db_url)\n",
    "    validated_gdf.to_postgis(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        dtype={'geometry': Geometry('GEOMETRY', srid=4326)}  # Specify the geometry type and SRID\n",
    "    )\n",
    "\n",
    "#set file path dictionary to retain data\n",
    "file_path_dict = {\n",
    "    'fire_danger': 'wfs_layers/fire_danger_2024-01-01.gpkg',\n",
    "    'active_fires': 'wfs_layers/active_fires_2024-01-01.gpkg',\n",
    "    'fire_history': 'wfs_layers/fire_history_2024_2024-01-01.gpkg',\n",
    "    'fire_history': 'wfs_layers/fire_history_ytd_2024-01-01.gpkg',\n",
    "    'fire_perimeter_estimates': 'wfs_layers/fire_perimeter_estimate_2024-01-01.gpkg',\n",
    "    'forecast_weather_stations': 'wfs_layers/forecast_weather_stations_2024-01-01.gpkg',\n",
    "    'm3_hotspots': 'wfs_layers/m3_hotspots_2024-01-01.gpkg',\n",
    "    'reporting_weather_stations': 'wfs_layers/reporting_weather_stations_2024-01-01.gpkg',\n",
    "    'reporting_weather_stations_forecast': 'wfs_layers/reporting_weather_stations_forecast_2024-01-01.gpkg'\n",
    "}\n",
    "\n",
    "for table, file_path in file_path_dict.items():\n",
    "    print(f'Processing table: {table} from file path: {file_path}')\n",
    "    load_gpkg_to_postgis(file_path, table)\n",
    "\n",
    "\n"
   ],
   "id": "1490130737c24010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: fire_danger from file path: wfs_layers/fire_danger_2024-01-01.gpkg\n",
      "2025-06-07 00:00:00\n",
      "Processing table: active_fires from file path: wfs_layers/active_fires_2024-01-01.gpkg\n",
      "2025-06-07 00:00:00\n",
      "Processing table: fire_history from file path: wfs_layers/fire_history_ytd_2024-01-01.gpkg\n",
      "Processing table: fire_perimeter_estimates from file path: wfs_layers/fire_perimeter_estimate_2024-01-01.gpkg\n",
      "2025-06-07 00:00:00\n",
      "Processing table: forecast_weather_stations from file path: wfs_layers/forecast_weather_stations_2024-01-01.gpkg\n",
      "Processing table: m3_hotspots from file path: wfs_layers/m3_hotspots_2024-01-01.gpkg\n",
      "Processing table: reporting_weather_stations from file path: wfs_layers/reporting_weather_stations_2024-01-01.gpkg\n",
      "Processing table: reporting_weather_stations_forecast from file path: wfs_layers/reporting_weather_stations_forecast_2024-01-01.gpkg\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e647c3bac34cfece"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
